# ü¶Å Simulaci√≥n Inteligente: Le√≥n vs. Impala
**Materia:** Sistemas Inteligentes ‚Äì Grupo 1754  
**Instituci√≥n:** UNAM ‚Ä¢ FES Acatl√°n  
**Alumnos** HERNANDEZ BARRAZA ALEJANDRO - PULIDO ZARI√ëAN BRIAN JOB - FRAUSTO HERNANDEZ LUIS ANGEL 
**Fecha:** Diciembre 2025  

## üìò Descripci√≥n del Proyecto
Este proyecto presenta una simulaci√≥n basada en t√©cnicas de Aprendizaje por Refuerzo donde un agente depredador (Le√≥n) debe aprender progresivamente a capturar a un Impala dentro de un entorno discreto.  
El comportamiento del agente se desarrolla sin reglas expl√≠citas: su estrategia emerge a partir de exploraci√≥n, recompensas, penalizaciones y mecanismos como Q-Learning simplificado, memoria retrospectiva y selecci√≥n de acciones epsilon-greedy.

## üöÄ Ejecuci√≥n del Software

### 1. Descarga
- Ubicar la carpeta **Entrega_Ejecutable** en el repositorio.  
- Descargar el archivo **Juego Final SI.rar** y moverlo a cualquier ubicaci√≥n conveniente.

### 2. Extracci√≥n
**No abrir el juego desde el archivo .rar.**

- Clic derecho en el .rar ‚Üí *Extraer aqu√≠* o *Extraer todo*.  
- Confirmar que la carpeta resultante incluya:  
  - `SimulacionLeon.exe`  
  - Carpeta `_Data`  
  - Carpeta `MonoBleedingEdge`

### 3. Ejecuci√≥n
- Abrir la carpeta descomprimida.  
- Ejecutar **SimulacionLeon.exe**.  
- Si aparece SmartScreen, seleccionar: *M√°s informaci√≥n* ‚Üí *Ejecutar de todas formas*.

## üéÆ Controles Disponibles
- **Modo Entrenamiento:** Acelera la simulaci√≥n para optimizar el aprendizaje.  
- **Reiniciar Sesi√≥n:** Reinicia la escena conservando el conocimiento adquirido.  
- **Salir:** Cierra la aplicaci√≥n.

## üß† Memoria del Agente
El comportamiento del depredador se genera din√°micamente mediante valores Q almacenados en un archivo JSON. Cada registro contiene:

- **Estado:** Combinaci√≥n de posici√≥n y estado del Impala.  
- **Acci√≥n:** Avanzar o permanecer oculto.  
- **Valor Q:** Evaluaci√≥n del beneficio esperado.

### Ubicaci√≥n del archivo de memoria
```
C:\Users\[USUARIO]\AppData\LocalLow\DefaultCompany\
SimulacionLeonImpala\cerebro_leon.json
```

### Reinicio total del aprendizaje
Para restaurar al agente a su estado inicial:  
1. Cerrar el programa.  
2. Eliminar el archivo `cerebro_leon.json`.  
3. Ejecutar nuevamente la simulaci√≥n.

## üìà Etapas de Aprendizaje del Le√≥n

### 1. Exploraci√≥n Inicial
- Acciones completamente aleatorias.  
- Tasa de √©xito baja debido a la ausencia de estrategia.

### 2. Ajuste Conductual
- Aprende que ciertos movimientos generan penalizaciones.  
- Mantiene conductas defensivas (ocultarse frecuentemente).

### 3. Comportamiento Avanzado
- Uso de epsilon-greedy para explorar rutas nuevas.  
- Aprendizaje retrospectivo que recompensa cadenas completas de decisiones.  
- Desarrollo de t√°cticas como flanqueo, sigilo y ataque oportuno.

## ‚öôÔ∏è Reglas Operativas de la Simulaci√≥n
- **Turnos discretos:** Ambos agentes act√∫an dentro del mismo ciclo.  
- **Visi√≥n del Impala:** Basada en raycasting con un √°ngulo de 45¬∞.  
- **Sigilo:** El Le√≥n oculto no puede ser detectado.  
- **Distancia cr√≠tica:** A menos de 3 unidades, el Impala huye autom√°ticamente.
